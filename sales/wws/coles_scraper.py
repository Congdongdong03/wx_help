def save_to_database(self, image_paths):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        try:
            connection = mysql.connector.connect(**DB_CONFIG)
            cursor = connection.cursor()
            
            # æ’å…¥æ–°æ•°æ®
            today = date.today()
            for page_number, file_path in image_paths:
                query = """
                INSERT INTO catalogue_images (store_name, page_number, image_data, week_date)
                VALUES (%s, %s, %s, %s)
                """
                cursor.execute(query, ('coles', page_number, file_path, today))
            
            connection.commit()
            logging.info(f"æˆåŠŸä¿å­˜ {len(image_paths)} æ¡Colesè®°å½•åˆ°æ•°æ®åº“")
            
            cursor.close()
            connection.close()
            return True
            
        except Exception as e:
            logging.error(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False#!/usr/bin/env python3
"""
Colesä¸“ç”¨çˆ¬è™« - å…ˆç¡®ä¿Colesèƒ½æ­£å¸¸å·¥ä½œ
"""

import os
import requests
import time
import logging
from datetime import datetime, date
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from pdf2image import convert_from_bytes
import mysql.connector
from config import DB_CONFIG

class ColesScraper:
    def __init__(self):
        self.setup_logging()
        self.driver = None
        self.images_dir = None  # æ·»åŠ è¿™ä¸ªå±æ€§
        self.ensure_directories()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('coles_scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def ensure_directories(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        # ä¿®æ”¹è·¯å¾„ï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„ä½ç½®åˆ›å»ºç›®å½•
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)  # çˆ¬è™«ç›®å½•çš„ä¸Šçº§ç›®å½•
        
        self.images_dir = os.path.join(project_root, 'public', 'catalogue_images', 'coles')
        os.makedirs(self.images_dir, exist_ok=True)
        
        logging.info(f"Coleså›¾ç‰‡å­˜å‚¨ç›®å½•: {self.images_dir}")
        logging.info("Coleså›¾ç‰‡å­˜å‚¨ç›®å½•å·²å‡†å¤‡å®Œæ¯•")
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)
            
            logging.info("Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logging.error(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            logging.info("æµè§ˆå™¨å·²å…³é—­")
    
    def get_coles_pdf_url(self):
        """è·å–Coles PDFé“¾æ¥ - ä¸“é—¨æ‰¾ 'This week's catalogue'"""
        try:
            logging.info("è®¿é—®Colesç›®å½•é¡µé¢...")
            self.driver.get("https://www.coles.com.au/catalogues")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾"This week's catalogue"æ–‡å­—ï¼Œç„¶åæ‰¾Download PDFæŒ‰é’®
            try:
                logging.info("æŸ¥æ‰¾ 'This week's catalogue' åŒºåŸŸ...")
                
                # æŸ¥æ‰¾åŒ…å«"This week's catalogue"çš„å…ƒç´ 
                this_week_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), \"This week's catalogue\")]")
                
                for element in this_week_elements:
                    logging.info("æ‰¾åˆ° 'This week's catalogue' æ–‡å­—")
                    
                    # å‘ä¸ŠæŸ¥æ‰¾çˆ¶å®¹å™¨
                    parent = element
                    for level in range(10):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾10å±‚
                        try:
                            # åœ¨å½“å‰å®¹å™¨ä¸­æŸ¥æ‰¾"Download PDF"æŒ‰é’®
                            download_buttons = parent.find_elements(By.XPATH, ".//*[contains(text(), 'Download PDF') or contains(text(), 'download pdf') or contains(text(), 'PDF')]")
                            
                            for button in download_buttons:
                                # å°è¯•è·å–é“¾æ¥
                                href = button.get_attribute('href')
                                if not href:
                                    # å¦‚æœä¸æ˜¯é“¾æ¥ï¼Œå¯èƒ½æ˜¯æŒ‰é’®ï¼ŒæŸ¥æ‰¾çˆ¶çº§é“¾æ¥
                                    button_parent = button.find_element(By.XPATH, "..")
                                    href = button_parent.get_attribute('href')
                                
                                if href and '.pdf' in href.lower():
                                    logging.info(f"é€šè¿‡'This week's catalogue'æ‰¾åˆ°PDF: {href}")
                                    return href
                            
                            # æŸ¥æ‰¾çˆ¶çº§
                            parent = parent.find_element(By.XPATH, "..")
                            
                        except:
                            break
                            
            except Exception as e:
                logging.info(f"æ–¹æ³•1å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰"Download PDF"æŒ‰é’®
            try:
                logging.info("æŸ¥æ‰¾æ‰€æœ‰ 'Download PDF' æŒ‰é’®...")
                
                download_selectors = [
                    "//*[contains(text(), 'Download PDF')]",
                    "//*[contains(text(), 'download pdf')]", 
                    "//*[contains(text(), 'PDF')]",
                    "a[href*='.pdf']",
                    "button[href*='.pdf']"
                ]
                
                for selector in download_selectors:
                    try:
                        if selector.startswith("//"):
                            elements = self.driver.find_elements(By.XPATH, selector)
                        else:
                            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        
                        for element in elements:
                            href = element.get_attribute('href')
                            if href and '.pdf' in href.lower():
                                logging.info(f"é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ°PDF: {href}")
                                return href
                                
                    except:
                        continue
                        
            except Exception as e:
                logging.info(f"æ–¹æ³•2å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥ï¼ˆå¤‡ç”¨ï¼‰
            try:
                logging.info("å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥...")
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in all_links:
                    href = link.get_attribute('href')
                    if href and '.pdf' in href.lower():
                        logging.info(f"å¤‡ç”¨æ–¹æ¡ˆæ‰¾åˆ°PDF: {href}")
                        return href
                        
            except Exception as e:
                logging.info(f"æ–¹æ³•3å¤±è´¥: {e}")
            
            logging.error("æœªæ‰¾åˆ° 'This week's catalogue' PDFé“¾æ¥")
            return None
            
        except Exception as e:
            logging.error(f"è·å–Coles PDFå¤±è´¥: {e}")
            return None
    
    def download_pdf(self, pdf_url):
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            logging.info("ä¸‹è½½Coles PDF...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(pdf_url, headers=headers, timeout=60)
            if response.status_code == 200:
                logging.info(f"PDFä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(response.content)} bytes")
                return response.content
            else:
                logging.error(f"PDFä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            logging.error(f"PDFä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def pdf_to_images(self, pdf_data):
        """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡"""
        try:
            logging.info("è½¬æ¢PDFä¸ºå›¾ç‰‡...")
            
            # ä½¿ç”¨pdf2imageè½¬æ¢PDF
            images = convert_from_bytes(
                pdf_data,
                dpi=150,  # å›¾ç‰‡è´¨é‡
                fmt='JPEG'
            )
            
            logging.info(f"PDFè½¬æ¢æˆåŠŸï¼Œå…± {len(images)} é¡µ")
            return images
            
        except Exception as e:
            logging.error(f"PDFè½¬æ¢å¤±è´¥: {e}")
            return []
    
    def save_images(self, images):
        """ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°å¹¶è¿”å›è·¯å¾„"""
        try:
            today = date.today()
            date_str = today.strftime('%Y%m%d')
            saved_paths = []
            
            for i, image in enumerate(images, 1):
                filename = f"{date_str}_page{i}.jpg"
                file_path = os.path.join(self.images_dir, filename)
                
                # ä¿å­˜å›¾ç‰‡
                image.save(file_path, 'JPEG', quality=85)
                
                # è®°å½•æ•°æ®åº“è·¯å¾„ï¼ˆç”¨äºAPIè¿”å›ï¼‰
                db_path = f"/catalogue_images/coles/{filename}"
                saved_paths.append((i, db_path))
                
                logging.info(f"ä¿å­˜å›¾ç‰‡: ç¬¬{i}é¡µ -> {file_path}")
            
            return saved_paths
            
        except Exception as e:
            logging.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return []
    
    def clean_old_files_and_data(self):
        """æ¸…é™¤æ—§çš„å›¾ç‰‡æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•"""
        try:
            logging.info("å¼€å§‹æ¸…ç†æ—§æ•°æ®...")
            
            # 1. å…ˆä»æ•°æ®åº“è·å–æ—§æ–‡ä»¶è·¯å¾„
            connection = mysql.connector.connect(**DB_CONFIG)
            cursor = connection.cursor()
            
            # æŸ¥è¯¢ç°æœ‰çš„Coleså›¾ç‰‡è®°å½•
            cursor.execute("SELECT image_data FROM catalogue_images WHERE store_name = 'coles'")
            old_records = cursor.fetchall()
            
            # 2. åˆ é™¤æ—§çš„å›¾ç‰‡æ–‡ä»¶
            deleted_files = 0
            for record in old_records:
                file_path = record[0]  # å¦‚: "/catalogue_images/coles/20250605_page1.jpg"
                
                # è½¬æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
                if file_path.startswith('/catalogue_images/'):
                    relative_path = file_path[1:]  # å»æ‰å¼€å¤´çš„ "/"
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    project_root = os.path.dirname(current_dir)
                    actual_file_path = os.path.join(project_root, 'public', relative_path)
                    
                    # åˆ é™¤æ–‡ä»¶
                    if os.path.exists(actual_file_path):
                        try:
                            os.remove(actual_file_path)
                            deleted_files += 1
                            logging.info(f"åˆ é™¤æ—§æ–‡ä»¶: {actual_file_path}")
                        except Exception as e:
                            logging.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {actual_file_path}: {e}")
            
            logging.info(f"åˆ é™¤äº† {deleted_files} ä¸ªæ—§å›¾ç‰‡æ–‡ä»¶")
            
            # 3. åˆ é™¤æ•°æ®åº“è®°å½•
            cursor.execute("DELETE FROM catalogue_images WHERE store_name = 'coles'")
            deleted_records = cursor.rowcount
            connection.commit()
            logging.info(f"åˆ é™¤äº† {deleted_records} æ¡æ—§æ•°æ®åº“è®°å½•")
            
            cursor.close()
            connection.close()
            
            return True
            
        except Exception as e:
            logging.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return False
    
    def run_scraper(self):
        """è¿è¡ŒColesçˆ¬è™«"""
        try:
            logging.info("=" * 50)
            logging.info("Colesç›®å½•çˆ¬è™«å¼€å§‹è¿è¡Œ")
            logging.info(f"è¿è¡Œæ—¶é—´: {datetime.now()}")
            logging.info("=" * 50)
            
            # æ¸…ç†æ—§æ•°æ®å’Œæ–‡ä»¶
            if not self.clean_old_files_and_data():
                logging.warning("æ¸…ç†æ—§æ•°æ®å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
            
            # å¯åŠ¨æµè§ˆå™¨
            if not self.setup_driver():
                logging.error("æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
                return False
            
            # è·å–PDF URL
            pdf_url = self.get_coles_pdf_url()
            if not pdf_url:
                logging.error("æœªæ‰¾åˆ°PDFé“¾æ¥")
                return False
            
            # ä¸‹è½½PDF
            pdf_data = self.download_pdf(pdf_url)
            if not pdf_data:
                logging.error("PDFä¸‹è½½å¤±è´¥")
                return False
            
            # è½¬æ¢ä¸ºå›¾ç‰‡
            images = self.pdf_to_images(pdf_data)
            if not images:
                logging.error("PDFè½¬æ¢å¤±è´¥")
                return False
            
            # ä¿å­˜å›¾ç‰‡
            image_paths = self.save_images(images)
            if not image_paths:
                logging.error("å›¾ç‰‡ä¿å­˜å¤±è´¥")
                return False
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if self.save_to_database(image_paths):
                logging.info("ğŸ‰ Colesçˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼")
                logging.info(f"å…±å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")
                logging.info("âœ… æ—§æ–‡ä»¶å·²æ¸…ç†ï¼Œæ–°å›¾ç‰‡å·²ä¿å­˜")
                return True
            else:
                logging.error("æ•°æ®åº“ä¿å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            logging.error(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
            return False
            
        finally:
            self.close_driver()

def main():
    """ä¸»å‡½æ•°"""
    scraper = ColesScraper()
    success = scraper.run_scraper()
    
    if success:
        logging.info("âœ… Colesçˆ¬è™«æ‰§è¡ŒæˆåŠŸ")
        logging.info("ğŸ¯ ç°åœ¨å¯ä»¥æµ‹è¯•API: http://localhost:3000/api/posts96")
    else:
        logging.error("âŒ Colesçˆ¬è™«æ‰§è¡Œå¤±è´¥")

if __name__ == "__main__":
    main()