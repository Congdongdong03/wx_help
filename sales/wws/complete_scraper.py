#!/usr/bin/env python3
"""
å®Œæ•´è¶…å¸‚ç›®å½•çˆ¬è™« - Coles + Woolworths
æ¯å‘¨ä¸‰è‡ªåŠ¨è¿è¡Œï¼Œçˆ¬å–PDFç›®å½•å¹¶è½¬æ¢ä¸ºå›¾ç‰‡
"""

import os
import requests
import time
import logging
from datetime import datetime, date
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from pdf2image import convert_from_bytes
import mysql.connector
from config import DB_CONFIG
import schedule

class SupermarketScraper:
    def __init__(self):
        self.setup_logging()
        self.driver = None
        self.ensure_directories()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('supermarket_scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def ensure_directories(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs('../public/catalogue_images/coles', exist_ok=True)
        os.makedirs('../public/catalogue_images/woolworths', exist_ok=True)
        logging.info("å›¾ç‰‡å­˜å‚¨ç›®å½•å·²å‡†å¤‡å®Œæ¯•")
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(10)
            
            logging.info("Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logging.error(f"æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            logging.info("æµè§ˆå™¨å·²å…³é—­")
    
    def scrape_coles_catalogue(self):
        """çˆ¬å–Colesç›®å½• - ä½¿ç”¨ç¬¬ä¸€ä¸ªPDFé“¾æ¥"""
        try:
            logging.info("å¼€å§‹çˆ¬å–Colesç›®å½•...")
            self.driver.get("https://www.coles.com.au/catalogues")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "a"))
            )
            
            # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
            all_links = self.driver.find_elements(By.TAG_NAME, "a")
            pdf_links = []
            for link in all_links:
                href = link.get_attribute('href')
                if href and '.pdf' in href.lower():
                    pdf_links.append(href)
            
            if pdf_links:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªPDFé“¾æ¥ï¼ˆé€šå¸¸æ˜¯ä¸»è¦ç›®å½•ï¼‰
                main_pdf = pdf_links[0]
                logging.info(f"æ‰¾åˆ°Colesä¸»ç›®å½•PDF: {main_pdf}")
                return main_pdf
            
            logging.error("æœªæ‰¾åˆ°Coles PDFé“¾æ¥")
            return None
            
        except Exception as e:
            logging.error(f"çˆ¬å–Colesç›®å½•å¤±è´¥: {e}")
            return None
    
    def scrape_woolworths_catalogue(self):
        """çˆ¬å–Woolworthsç›®å½• - å…ˆè®¾ç½®é‚®ç¼–å†æ‰¾PDF"""
        try:
            logging.info("å¼€å§‹çˆ¬å–Woolworthsç›®å½•...")
            self.driver.get("https://www.woolworths.com.au/shop/catalogue")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # å°è¯•è®¾ç½®é‚®ç¼–ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            try:
                # æŸ¥æ‰¾é‚®ç¼–è¾“å…¥æ¡†
                postcode_selectors = [
                    'input[placeholder*="postcode"]',
                    'input[placeholder*="Postcode"]', 
                    'input[name*="postcode"]',
                    'input[id*="postcode"]'
                ]
                
                postcode_input = None
                for selector in postcode_selectors:
                    try:
                        postcode_input = self.driver.find_element(By.CSS_SELECTOR, selector)
                        break
                    except:
                        continue
                
                if postcode_input:
                    logging.info("æ‰¾åˆ°é‚®ç¼–è¾“å…¥æ¡†ï¼Œè®¾ç½®é»˜è®¤é‚®ç¼–...")
                    postcode_input.clear()
                    postcode_input.send_keys("2000")  # æ‚‰å°¼CBDé‚®ç¼–
                    
                    # æŸ¥æ‰¾æäº¤æŒ‰é’®
                    submit_selectors = [
                        'button[type="submit"]',
                        'button:contains("Submit")',
                        'button:contains("Go")',
                        '.submit-button'
                    ]
                    
                    for selector in submit_selectors:
                        try:
                            submit_btn = self.driver.find_element(By.CSS_SELECTOR, selector)
                            submit_btn.click()
                            logging.info("å·²æäº¤é‚®ç¼–")
                            time.sleep(3)  # ç­‰å¾…é¡µé¢æ›´æ–°
                            break
                        except:
                            continue
                            
            except Exception as e:
                logging.info(f"è®¾ç½®é‚®ç¼–å¤±è´¥ï¼Œç»§ç»­æŸ¥æ‰¾PDF: {e}")
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            all_links = self.driver.find_elements(By.TAG_NAME, "a")
            pdf_links = []
            for link in all_links:
                href = link.get_attribute('href')
                if href and '.pdf' in href.lower():
                    pdf_links.append(href)
            
            if pdf_links:
                logging.info(f"æ‰¾åˆ°Woolworths PDF: {pdf_links[0]}")
                return pdf_links[0]
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            logging.info("å°è¯•æŸ¥æ‰¾å…¶ä»–æ ¼å¼çš„ç›®å½•é“¾æ¥...")
            
            # æŸ¥æ‰¾å¯èƒ½çš„ç›®å½•é¡µé¢é“¾æ¥
            catalogue_links = []
            for link in all_links:
                href = link.get_attribute('href')
                if href and ('catalogue' in href.lower() or 'catalog' in href.lower()):
                    catalogue_links.append(href)
            
            if catalogue_links:
                logging.info(f"æ‰¾åˆ° {len(catalogue_links)} ä¸ªç›®å½•ç›¸å…³é“¾æ¥")
                # è®¿é—®ç¬¬ä¸€ä¸ªç›®å½•é“¾æ¥ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰PDF
                self.driver.get(catalogue_links[0])
                time.sleep(3)
                
                # å†æ¬¡æŸ¥æ‰¾PDF
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in all_links:
                    href = link.get_attribute('href')
                    if href and '.pdf' in href.lower():
                        logging.info(f"åœ¨å­é¡µé¢æ‰¾åˆ°Woolworths PDF: {href}")
                        return href
            
            logging.error("æœªæ‰¾åˆ°Woolworths PDFé“¾æ¥")
            return None
            
        except Exception as e:
            logging.error(f"çˆ¬å–Woolworthsç›®å½•å¤±è´¥: {e}")
            return None
    
    def download_pdf(self, pdf_url, store_name):
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            logging.info(f"æ­£åœ¨ä¸‹è½½ {store_name} PDF...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(pdf_url, headers=headers, timeout=60)
            if response.status_code == 200:
                logging.info(f"{store_name} PDFä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(response.content)} bytes")
                return response.content
            else:
                logging.error(f"{store_name} PDFä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            logging.error(f"{store_name} PDFä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def pdf_to_images(self, pdf_data, store_name):
        """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡"""
        try:
            logging.info(f"æ­£åœ¨è½¬æ¢ {store_name} PDFä¸ºå›¾ç‰‡...")
            
            # ä½¿ç”¨pdf2imageè½¬æ¢PDF
            images = convert_from_bytes(
                pdf_data,
                dpi=150,  # å›¾ç‰‡è´¨é‡
                fmt='JPEG'
            )
            
            logging.info(f"{store_name} PDFè½¬æ¢æˆåŠŸï¼Œå…± {len(images)} é¡µ")
            return images
            
        except Exception as e:
            logging.error(f"{store_name} PDFè½¬æ¢å¤±è´¥: {e}")
            return []
    
    def save_images_to_disk(self, images, store_name):
        """ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°ç£ç›˜"""
        try:
            today = date.today()
            date_str = today.strftime('%Y%m%d')
            saved_paths = []
            
            for i, image in enumerate(images, 1):
                filename = f"{date_str}_page{i}.jpg"
                file_path = f"../public/catalogue_images/{store_name}/{filename}"
                
                # ä¿å­˜å›¾ç‰‡
                image.save(file_path, 'JPEG', quality=85)
                
                # è®°å½•æ•°æ®åº“è·¯å¾„
                db_path = f"/catalogue_images/{store_name}/{filename}"
                saved_paths.append((i, db_path))
                
                logging.info(f"ä¿å­˜æˆåŠŸ: {store_name} ç¬¬{i}é¡µ -> {file_path}")
            
            return saved_paths
            
        except Exception as e:
            logging.error(f"ä¿å­˜ {store_name} å›¾ç‰‡å¤±è´¥: {e}")
            return []
    
    def save_to_database(self, store_name, image_paths):
        """ä¿å­˜å›¾ç‰‡è·¯å¾„åˆ°æ•°æ®åº“"""
        try:
            connection = mysql.connector.connect(**DB_CONFIG)
            cursor = connection.cursor()
            
            # æ¸…é™¤æ—§æ•°æ®
            cursor.execute("DELETE FROM catalogue_images WHERE store_name = %s", (store_name,))
            deleted_count = cursor.rowcount
            logging.info(f"åˆ é™¤äº† {deleted_count} æ¡æ—§çš„ {store_name} è®°å½•")
            
            # æ’å…¥æ–°æ•°æ®
            today = date.today()
            for page_number, file_path in image_paths:
                query = """
                INSERT INTO catalogue_images (store_name, page_number, image_data, week_date)
                VALUES (%s, %s, %s, %s)
                """
                cursor.execute(query, (store_name, page_number, file_path, today))
            
            connection.commit()
            logging.info(f"æˆåŠŸä¿å­˜ {len(image_paths)} æ¡ {store_name} è®°å½•åˆ°æ•°æ®åº“")
            
            cursor.close()
            connection.close()
            return True
            
        except Exception as e:
            logging.error(f"ä¿å­˜ {store_name} æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def scrape_store(self, store_name):
        """çˆ¬å–å•ä¸ªå•†åº—çš„ç›®å½•"""
        try:
            logging.info(f"å¼€å§‹å¤„ç† {store_name}...")
            
            # è·å–PDF URL
            if store_name == 'coles':
                pdf_url = self.scrape_coles_catalogue()
            elif store_name == 'woolworths':
                pdf_url = self.scrape_woolworths_catalogue()
            else:
                logging.error(f"æœªçŸ¥çš„å•†åº—åç§°: {store_name}")
                return False
            
            if not pdf_url:
                logging.error(f"æœªæ‰¾åˆ° {store_name} çš„PDFé“¾æ¥")
                return False
            
            # ä¸‹è½½PDF
            pdf_data = self.download_pdf(pdf_url, store_name)
            if not pdf_data:
                return False
            
            # è½¬æ¢ä¸ºå›¾ç‰‡
            images = self.pdf_to_images(pdf_data, store_name)
            if not images:
                return False
            
            # ä¿å­˜å›¾ç‰‡åˆ°ç£ç›˜
            image_paths = self.save_images_to_disk(images, store_name)
            if not image_paths:
                return False
            
            # ä¿å­˜è·¯å¾„åˆ°æ•°æ®åº“
            if self.save_to_database(store_name, image_paths):
                logging.info(f"âœ… {store_name} å¤„ç†å®Œæˆï¼")
                return True
            else:
                return False
                
        except Exception as e:
            logging.error(f"å¤„ç† {store_name} å¤±è´¥: {e}")
            return False
    
    def run_full_scraper(self):
        """è¿è¡Œå®Œæ•´çˆ¬è™«"""
        try:
            logging.info("=" * 60)
            logging.info("è¶…å¸‚ç›®å½•çˆ¬è™«å¼€å§‹è¿è¡Œ")
            logging.info(f"è¿è¡Œæ—¶é—´: {datetime.now()}")
            logging.info("=" * 60)
            
            # å¯åŠ¨æµè§ˆå™¨
            if not self.setup_driver():
                logging.error("æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼Œçˆ¬è™«åœæ­¢")
                return
            
            # çˆ¬å–Coles
            coles_success = self.scrape_store('coles')
            time.sleep(5)  # é—´éš”
            
            # çˆ¬å–Woolworths
            woolworths_success = self.scrape_store('woolworths')
            
            # æ€»ç»“
            if coles_success and woolworths_success:
                logging.info("ğŸ‰ æ‰€æœ‰å•†åº—çˆ¬å–æˆåŠŸï¼")
            elif coles_success or woolworths_success:
                logging.info("âš ï¸ éƒ¨åˆ†å•†åº—çˆ¬å–æˆåŠŸ")
            else:
                logging.error("âŒ æ‰€æœ‰å•†åº—çˆ¬å–å¤±è´¥")
            
        except Exception as e:
            logging.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            
        finally:
            self.close_driver()

def scheduled_job():
    """å®šæ—¶ä»»åŠ¡"""
    logging.info("å®šæ—¶ä»»åŠ¡è§¦å‘ï¼šå¼€å§‹æ‰§è¡Œçˆ¬è™«")
    scraper = SupermarketScraper()
    scraper.run_full_scraper()

def main():
    """ä¸»å‡½æ•°"""
    # ç«‹å³è¿è¡Œä¸€æ¬¡ï¼ˆæµ‹è¯•ç”¨ï¼‰
    logging.info("ç«‹å³è¿è¡Œä¸€æ¬¡çˆ¬è™«ï¼ˆæµ‹è¯•ï¼‰...")
    scraper = SupermarketScraper()
    scraper.run_full_scraper()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯å‘¨ä¸‰è¿è¡Œ
    schedule.every().wednesday.at("10:00").do(scheduled_job)
    logging.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼šæ¯å‘¨ä¸‰10:00è‡ªåŠ¨è¿è¡Œ")
    
    # ä¿æŒç¨‹åºè¿è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    main()